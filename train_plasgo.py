import sys
from transformers import BertConfig, EvalPrediction
from sklearn.metrics import average_precision_score
import numpy as np
from datasets import load_dataset
from transformers import TrainingArguments, Trainer
import torch, sys, os, csv, argparse
import pickle as pkl
from collections import defaultdict
from model import plasgo_model


#############################################################
########################  Parameters  #######################
#############################################################
parser = argparse.ArgumentParser(description="""PlasGO is a Python library for predicting GO terms of plasmid-encoded proteins.
                                 PlasGO is designed as a hierarchical architecture that leverages the powerful foundation protein language model (ProtTrans-ProtT5) to learn the local context within protein sentences and a BERT model to capture the global context within plasmid sentences.""")
parser.add_argument('--model_path', help='folder to store your customized models, default: custom_models', type=str, default='custom_models')
parser.add_argument('--data', help='folder to store the training data (generated by prepare_training_data.py), default: training_data', type=str, default='training_data')
parser.add_argument('--device', help="device utilized for training ('gpu' or 'cpu'), default: 'gpu'", type=str, default = 'gpu')
parser.add_argument('--threads', help="number of threads utilized for training if 'cpu' is detected ('cuda' not found), default: 2", type=int, default=2)
parser.add_argument('--lr', help="learning rate for training the models, default: 1e-4", type=float, default=1e-4)
parser.add_argument('--batch_size', help="batch size for training the models, default: 32", type=int, default=32)
parser.add_argument('--epoch_num', help="number of epochs for training the models, default: 10", type=int, default=10)
inputs = parser.parse_args()
data_fn = inputs.data


#############################################################
########################  Help info  ########################
#############################################################
def help_info():
    print('')
    print("""Usage of train_plasgo.py:
        [--model_path MODEL_PATH]   folder to store your customized models, default: custom_models
        [--data DATA]   folder to store the training data (generated by prepare_training_data.py), default: training_data
        [--device DEVICE]   device utilized for training ('gpu' or 'cpu'), default: 'gpu'
        [--threads THREADS] number of threads utilized for training if 'cpu' is detected ('cuda' not found), default: 2
        [--lr LR]   learning rate for training the models, default: 1e-4
        [--batch_size BATCH_SIZE]   batch size for training the models, default: 32
        [--epoch_num EPOCH_NUM] number of epochs for training the models, default: 10
    """)


#############################################################
######################  Check folders  ######################
#############################################################
data_fn = inputs.data
if not os.path.exists(data_fn):
    print(f"Error! The training data folder '{data_fn}' is unavailable. Please use the option '--data' to indicate the directory where the training data files generated by 'prepare_training_data.py' are stored.")
    help_info()
    sys.exit()

mdl_fn = inputs.model_path
if not os.path.exists(mdl_fn):
    os.mkdir(mdl_fn)
os.system(f'cp {data_fn}/labels.dict {mdl_fn}/')
os.system(f'cp {data_fn}/rep_go.dict {mdl_fn}/')


#############################################################
##########################  Train  ##########################
#############################################################
def read_protein_ids(val_csv):
    protein_ids = []
    with open(val_csv, 'r') as file:
        csv_reader = csv.DictReader(file)
        for row in csv_reader:
            for column, value in row.items():
                if(column=='proteins'):
                    protein_ids += value.split(';')
    protein_ids = [i for i in protein_ids if ('*' not in i and i!='None')]
    return protein_ids


def preprocess_data(examples):
    encoding = {}
    # protein indexes
    proteins = examples["proteins"]
    protein_idxes = []
    for sentence in proteins:
        prots = sentence.split(';')
        tmp_list = []
        for prot in prots:
            if(prot == 'None'):
                tmp_list.append(0)
                continue
            if('*' in prot):
                tmp = prot[1:]
            else:
                tmp = prot
            tmp_list.append(prot_idx[tmp])
        protein_idxes.append(tmp_list)
    encoding['proteins'] = protein_idxes
    # labels
    labels_list = examples["labels"]
    labels_matrix = np.zeros((len(labels_list)*max_len, label_num))
    idx = 0
    for labels_text in labels_list:
        d = labels_text.split(';')
        for tmp in d:
            if(tmp=='None'):
                labels_matrix[idx, :] = -100
            else:
                x = tmp.split(' ')
                x = [float(i) for i in x]
                x = np.array(x)
                labels_matrix[idx] = x
            idx+=1
    labels_matrix = labels_matrix.reshape((len(labels_list), max_len, label_num))
    encoding["labels"] = labels_matrix.tolist()
    return encoding


# select the best model
def multi_label_metrics(predictions, labels):
    probs = torch.Tensor(predictions).to('cpu')
    probs = torch.reshape(probs, (-1, label_num))
    probs = sigmoid(probs)
    y_true = labels
    y_true = y_true.reshape(-1, label_num)

    # load all predictions
    idx = 0
    pred_dict = defaultdict(list)
    label_dict = {}
    for pred, label in zip(probs, y_true):
        if(-100 in label):
            continue
        protein_id = protein_ids[idx]
        idx+=1
        pred_dict[protein_id].append(pred)
        label_dict[protein_id] = label

    # average probs for each protein
    for protein in pred_dict:
        if(len(pred_dict[protein])==1):
            pred_dict[protein] = pred_dict[protein][0]
            continue
        stacked_tensor = torch.stack(pred_dict[protein])
        stacked_tensor = stacked_tensor.float()
        avg_tensor = torch.mean(stacked_tensor, dim=0)
        pred_dict[protein] = avg_tensor
    probs, y_true = [], []
    for prot in pred_dict:
        probs.append(pred_dict[prot])
        y_true.append(label_dict[prot])
    probs = np.concatenate(probs, axis=0).reshape(len(pred_dict), -1)
    y_true = np.concatenate(y_true, axis=0).reshape(len(pred_dict), -1)

    # delete zero-columns
    deleted_idx = []
    for i in range(y_true.shape[1]):
        true_labels_i = y_true[:, i]
        if(np.all(true_labels_i == 0)):
            deleted_idx.append(i)
    probs = np.delete(probs, deleted_idx, axis=1)
    y_true = np.delete(y_true, deleted_idx, axis=1)

    # Fmax
    f1s = []
    for threshold in np.arange(0, 1.01, 0.01):
        y_pred = np.zeros(probs.shape)
        y_pred[np.where(probs >= threshold)] = 1
        prs, rcs = [], []
        for pred, label in zip(y_pred, y_true):
            pred_num = np.count_nonzero(pred == 1)
            ans_num = np.count_nonzero(label == 1)
            if(ans_num==0):
                continue
            if(pred_num==0):
                rcs.append(0)
                continue
            else:
                identical = np.count_nonzero((pred==label)&(pred!=0))
                pr = identical/pred_num
                rc = identical/ans_num
            prs.append(pr)
            rcs.append(rc)
        avg_precision = np.mean(prs)
        avg_recall = np.mean(rcs)
        avg_f1 = (2*avg_precision*avg_recall)/(avg_precision+avg_recall)
        f1s.append(avg_f1)
    
    # AUPR
    num_labels = y_true.shape[1]
    aupr_scores = []
    for i in range(num_labels):
        true_labels_i = y_true[:, i]
        predicted_probabilities_i = probs[:, i]
        aupr_scores.append(average_precision_score(true_labels_i, predicted_probabilities_i))
    mean_aupr = np.mean(aupr_scores)

    combined_score = 0.7*max(f1s) + 0.3*mean_aupr   # weighted average
    metrics = {'Fmax': max(f1s), 'AUPR':mean_aupr, 'combined_score':combined_score}
    return metrics


def compute_metrics(p: EvalPrediction):
    preds = p.predictions[0] if isinstance(p.predictions,
            tuple) else p.predictions
    result = multi_label_metrics(
        predictions=preds,
        labels=p.label_ids)
    return result


prot_idx = pkl.load(open(f'{data_fn}/protein_embeddings/prot2idx.dict', 'rb'))
embeddings = pkl.load(open(f'{data_fn}/protein_embeddings/prot_embeddings.pkl', 'rb'))
for Class in ['mf', 'bp', 'cc']:
    # hyperparameter
    layer_num = 4 if Class in ['bp', 'mf'] else 2
    max_len = 56
    learning_rate = inputs.lr
    batch_size = inputs.batch_size
    num_epochs = inputs.epoch_num
    hidden_size = 512
    dropout_rate = 0.1 if Class in ['bp', 'mf'] else 0.2

    # load dataset
    train_path = f'{data_fn}/dataset_{Class}/training.csv'
    val_path = f'{data_fn}/dataset_{Class}/validation.csv'
    dataset = load_dataset('csv', data_files={'train': train_path, 'validation':val_path})
    label_num = len(pkl.load(open(f'{data_fn}/labels.dict', 'rb'))[Class])
    OUTPUT_dir = f'{mdl_fn}/{Class}/'
    if not os.path.exists(OUTPUT_dir):
        os.mkdir(OUTPUT_dir)

    # load the BERT configuration for the global BERT
    config = BertConfig(
        vocab_size=2,   # not in use
        hidden_size=hidden_size,
        num_hidden_layers=layer_num,
        num_attention_heads=8,
        max_position_embeddings=max_len,
        num_labels=label_num,
        intermediate_size = hidden_size*4,
        hidden_dropout_prob = dropout_rate,
        attention_probs_dropout_prob = dropout_rate
    )

    # initialize model
    model = plasgo_model(config, Class, raw_embed=embeddings)
    print(Class, 'No. of parameters:', model.num_parameters())
    sigmoid = torch.nn.Sigmoid()

    # protein ids for validation
    protein_ids = read_protein_ids(val_path)
    print(f'Proteins for validation: {len(set(protein_ids))}')

    encoded_dataset = dataset.map(preprocess_data,
                                batched=True,
                                num_proc=inputs.threads,
                                remove_columns=dataset['train'].column_names,
                                keep_in_memory=True)
    encoded_dataset.set_format("torch")
    print(encoded_dataset)

    warmup_step_num = int(len(encoded_dataset['train'])/batch_size)*num_epochs
    warmup_step_num = int(0.05*warmup_step_num)
    print(f'Warmup steps: {warmup_step_num}')

    args = TrainingArguments(
        output_dir=OUTPUT_dir,
        evaluation_strategy = 'epoch',
        save_strategy = "epoch",
        learning_rate=learning_rate,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=num_epochs,
        weight_decay=0.01,
        load_best_model_at_end=True,
        warmup_steps=warmup_step_num,
        metric_for_best_model="combined_score",
        save_total_limit=1,
        dataloader_drop_last = False,
    )

    trainer = Trainer(
        model,
        args,
        train_dataset=encoded_dataset["train"],
        eval_dataset=encoded_dataset["validation"],
        compute_metrics=compute_metrics)

    trainer.train()
    trainer.save_model(OUTPUT_dir)
    print(f'Training for GO category {Class} finished!')

